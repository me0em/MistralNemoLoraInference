{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f17d775-5bc2-46f6-9fc4-56d44073ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import yaml\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import pipeline\n",
    "from peft import get_peft_model, PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c908f6-a74e-42d1-83ac-1b9e065f3c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = Box(yaml.safe_load(file))\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = config.hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0759e29c-d3ed-4f16-acd2-48b1da250946",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.lora_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# max_seq_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c12b2b-389d-46e8-9993-957398c0d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    token=config.hf_token,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97dcb33-3585-4de0-b2ae-3277fb924745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(131074, 5120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(\n",
    "    len(tokenizer),\n",
    "    mean_resizing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6a0445-98af-4682-8568-c65f15317cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2ef71a-16b1-4bc2-8a4a-f4d344090673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, config.lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da462b-db7c-49c5-b065-4195e0456afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9577946-0e58-447b-8d15-7bec2eb2af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model: AutoModelForCausalLM,\n",
    "                  tokenizer: AutoTokenizer,\n",
    "                  query: str,\n",
    "                  device='cuda') -> str:\n",
    "    \"\"\" Tokenize query, pass it to the model,\n",
    "    convert tokens to human language text and return it\n",
    "    \"\"\"\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        # padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_dict=True,\n",
    "        add_generation_prompt=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # attention_mask = (\n",
    "    #     inputs[\"input_ids\"] != tokenizer.pad_token_id\n",
    "    # ).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            # attention_mask=attention_mask,\n",
    "            # max_length=100,\n",
    "            max_new_tokens=200,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    generated_text = generated_text[len(query):]    \n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eea20af-3b2e-4e06-a035-6036b67115c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your task is to answer the user's questions using only the information provided in the documents. Be sure to include a list of relevant documents ids that you used to formulate your answer.Context:{\"content\": [{\"doc_id\": \"666\", \"content\": \"В 2024 году американские выборы выиграл Трамп\"}, {\"doc_id\": \"1312\", \"content\": \"В 2020 году американские выборы выиграл Байден\"}]}\n",
      "Question:кто выиграл американские выборы в 2034 году?\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Your task is to answer the user's questions using only the information provided in the documents. Be sure to include a list of relevant documents ids that you used to formulate your answer.Context:{\"content\": [{\"doc_id\": \"666\", \"content\": \"В 2024 году американские выборы выиграл Трамп\"}, {\"doc_id\": \"1312\", \"content\": \"В 2020 году американские выборы выиграл Байден\"}]}\n",
    "Question:кто выиграл американские выборы в 2034 году?\"\"\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e23cdc99-7147-4a80-9ec9-2ef1078ed87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.97 s, sys: 7.83 ms, total: 4.98 s\n",
      "Wall time: 4.97 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' году?assistant\\n\\nК сожалению, я не могу найти информацию о том, кто выиграл американские выборы в 2034 году, используя данные, доступные мне. Моя способность отвечать ограничена информацией, которую я могу найти в базе знаний.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "generate_text(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    query=query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a08cf-7037-4266-ad62-775013f4e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49e78f-f71d-4887-8f14-8d775d96fe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb122f00-6715-400a-98b1-03a8ec1f1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = PeftConfig.from_pretrained(config.lora_path)\n",
    "# model = get_peft_model(model, config)\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e49f2-447c-4e41-9638-75b8be28e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "lora_path = \"models/sft-grndmrag-mistral-nemo-lora-128-2/checkpoint-10200\"# \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.tie_weights()\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
